First, I read the article about your experiment and explored the GitHub repo to better understand your workflow and read your prompts.

Next I implemented a simple Inspect agent in the file main.py using the Inspect builtin React() agent. I decided to use it because your agent scaffolding is very bulky, and I have no time today to implement it, and also I decided that it's a reasonable trade-off since you mentioned in the article that it's unnecessary complex for historic reasons, and the demonstration would be more convincing with a simpler agent.

I decided to use for the first version of MVP Inspect built-in model_grader_qa function, but it doesn’t allow grading in 4 stages as in your setup, so I decided to compress all of them into one prompt and let grader choose from all of your outcome options simultaneously. I latter implemented a more sophisticated scorer, but I left the prompt for this grader in the code under the name MODEL_GRADED_QA_PROMPT, and left the code for this solution in the comments

I also functionally recreated your setup for running a chess game using bash in the file chess_game.py

I used Docker for running the agent in the container, but I encountered an issue: attempts to execute bash commands caused this container to crash and evaluations were unable to properly run, so I didn't implement proper containerisation in this MVP. The problem is that Inspect requires to use a Docker container to run bash commands with its built-in function.

To test if I had an bug in my code, or the issue is with something else, I tried to run on the server my other evaluations that also use Inspect built-in bash command, that worked fine on my setup (I couldn't use it today), but the container crushed with them as well, so I concluded that the issue is probably not in the code, but since I had limited time, and spend too much of it on investigating this issue, I decided to look for alternative solutions.

I asked Denis Volk to set up another server with different configuration to test if server configuration is the reason for the failure, and Denis proposed to write my custom function to execute bash commands. I implemented this alternative solution, and evaluations worked just fine.

Then I executed a new 4-staged model grader reflecting the grader from the paper, by writing the stockfish_scorer function. It's a bit big and messy, and I had to delve into Inspect guts to implement it, but it seems to work.

To implement majority voting of scorer models, I tried to use Inspect builtin function multi_scorer, but for some reason Inspect did not recognize this function as a scorer, and reading of the Inspect source code, documentation and googling didn't help me to figure out how to solve the problem, so I've left 5 scorers running in parallel without aggregating the result. If I had more time, I would implement the solution by hand or figure out what's wrong with my calling of multi_scorer.

Then I run the evals on o1-preview and GPT-4o (twice with each model) and saved the Inspect logs to the logs folder


Next steps:
- My grader prompts are rather basic and graders often are all over the place about the outcome of the games, and GPT-4o sometimes makes illegal moves which graders misattribute to cheating, so the next logical step is to improve these prompts.

- Solve issues related to Docker, so the script would work properly, so there is no need to manually delete game files after each run and there are no risks of LLMs messing with the system.

- Implement a solution for proper majority voting of scorers.

- At the end of the day I figured out that it would be a good idea to include the contents of reasoning tokens of reasoning models into logs, but fast look at documentation didn’'t help to figure out how to do it, and I had no time on figuring that out, so this is also a next step.

- Inspect wants to convert the results to a qantitative metric, while the nature of the evals is such that there are several options to cheat, and it's reasonable to implement a complex metric for detecting cheating, but due to limited time I didn't engage with this issue. 
